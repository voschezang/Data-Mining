{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "import collections, copy, pickle\n",
    "from termcolor import colored\n",
    "from importlib import reload\n",
    "import gc\n",
    "from dateutil.parser import parse\n",
    "import scipy.linalg, scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 14\n",
    "# rcParams['text.usetex'] = True\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "import sklearn.linear_model\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the training dataset\n",
    "# Split the data up in n sets, based on `search_id` to prevent occurence of a `search_id` in multiple sets\n",
    "#   CF score-matrices can be computed here as well\n",
    "# Use crossvalidation on the n sets to select hyperparams\n",
    "# Finally train model on full training dataset and make a prediction of the (unseen) test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/training_set_VU_DM_clean.csv', sep=';', nrows=1*1000)\n",
    "data_test = pd.read_csv('data/test_set_VU_DM_clean.csv', sep=';', nrows=1000)\n",
    "# scores = pd.read_csv('data/scores_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['position'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm  1.0 comp1_rate\n",
      "rm  1.0 comp1_inv\n",
      "rm  1.0 comp1_rate_percent_diff\n",
      "rm  0.55 comp2_rate\n",
      "rm  0.523 comp2_inv\n",
      "rm  0.882 comp2_rate_percent_diff\n",
      "rm  0.6 comp3_rate\n",
      "rm  0.586 comp3_inv\n",
      "rm  0.904 comp3_rate_percent_diff\n",
      "rm  0.913 comp4_rate\n",
      "rm  0.906 comp4_inv\n",
      "rm  0.967 comp4_rate_percent_diff\n",
      "rm  0.482 comp5_rate\n",
      "rm  0.453 comp5_inv\n",
      "rm  0.827 comp5_rate_percent_diff\n",
      "rm  0.919 comp6_rate\n",
      "rm  0.913 comp6_inv\n",
      "rm  0.967 comp6_rate_percent_diff\n",
      "rm  0.895 comp7_rate\n",
      "rm  0.883 comp7_inv\n",
      "rm  0.955 comp7_rate_percent_diff\n",
      "rm  0.562 comp8_rate\n",
      "rm  0.554 comp8_inv\n",
      "rm  0.889 comp8_rate_percent_diff\n"
     ]
    }
   ],
   "source": [
    "for k in data.columns:\n",
    "    if data[k].isna().sum() > 0:\n",
    "        print('rm ', data[k].isna().sum() / data.shape[0], k)\n",
    "        data.drop(columns=[k], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = sklearn.utils.shuffle(data.srch_id.unique(), random_state=123)\n",
    "# ids = data.srch_id.unique()\n",
    "N = ids.size\n",
    "N # total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click_bool', 'booking_bool', 'score']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = [ k for k in data.columns if k not in data_test.columns ]\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_data = data[['click_bool', 'booking_bool', 'score']].copy()\n",
    "# x_data = data.drop(columns=y_labels)\n",
    "# TODO split xy at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear memory\n",
    "# data = None\n",
    "# gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "n = 5 # number of cv splits\n",
    "selection_size = np.floor(N/n).astype(int)\n",
    "a = selection_size\n",
    "# note that the final splice may be smaller\n",
    "ids_selections = [ ids[i*a: min((i+1)*a, N)] for i in range(n) ]\n",
    "assert len(ids_selections) == n\n",
    "print(selection_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_splits_indices = [ data.loc[data.srch_id.isin(srch_ids)].index for srch_ids in ids_selections ]\n",
    "data_splits = [ data.loc[data.srch_id.isin(srch_ids)] for srch_ids in ids_selections ]\n",
    "# x_data_splits = [ x_data.loc[x_data.srch_id.isin(srch_ids)] for srch_ids in ids_selections ]\n",
    "# y_data_splits = [ y_data.iloc[x.index] for x in x_data_splits ]\n",
    "# y_data_splits = [ y_data.loc[y_data.srch_id.isin(srch_ids)] for srch_ids in ids_selections ]\n",
    "# len(data_splits_indices)\n",
    "len(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now use (n-1) of the splits to train and the one other split to validate\n",
    "# Now we will sample from a split to prevent class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split slices again, into classes: select 1/3 booking, 1/3 click (but no booking), 1/3 none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_data_splits[0]\n",
    "# y_train = y_data_splits[0]\n",
    "# assert x_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bookings_clicks_others(data):\n",
    "    bookings = data.query('booking_bool == 1')\n",
    "    clicks = data.query('click_bool == 1 and booking_bool != 0')\n",
    "    others = data.query('click_bool != 1')\n",
    "    return bookings, clicks, others\n",
    "    \n",
    "def sample(datasets=[], size_per_sample=100):\n",
    "    sample_indices = [ np.random.choice(data.index, size_per_sample)\n",
    "                      for data in datasets\n",
    "                     ]\n",
    "    # TODO should concatenation be shuffled?\n",
    "    # sklearn.utils.shuffle\n",
    "    return np.concatenate(sample_indices)\n",
    "#     samples = [ data.loc[np.random.choice(data.index, size_per_sample)] \n",
    "#                 for data in datasets\n",
    "#               ]\n",
    "#     ordered_dataset_samples = pd.concat(samples)\n",
    "#     return ordered_dataset_samples.sample(frac=1)\n",
    "\n",
    "def cv_folds(bco_splits):\n",
    "    # generate an iterable that yields (train, test) incides\n",
    "    # :bco_splits = list of (bookings, clicks, others)\n",
    "    folds = []\n",
    "    for bco in bco_splits:\n",
    "        n_max = max([df.shape[0] for df in bco_split ])\n",
    "        fold_indices = sample(bco, n_max)\n",
    "        folds.append((cv_fold_indices, cv_fold_indices))\n",
    "    return folds\n",
    "    \n",
    "def split_xy(data: pd.DataFrame):\n",
    "    return data.drop(columns=y_labels).values, data['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "bco_splits = [ split_bookings_clicks_others(data) for data in data_splits ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 4)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. for every cv split i\n",
    "i = 0\n",
    "bco_split = bco_splits[i]\n",
    "bookings, clicks, others = bco_split\n",
    "n_max = max([xy.shape[0] for xy in bco_split ])\n",
    "n_min = min([xy.shape[0] for xy in bco_split ])\n",
    "n_max, n_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([274, 274, 988, 274, 134,  50])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample([bookings, clicks, others], size_per_sample=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bco_splits = [ split_bookings_clicks_others(data) for data in data_splits ]\n",
    "folds = cv_folds(bco_splits)\n",
    "x_train, y_train = split_xy(data)\n",
    "# model = sklearn.ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# cross_val_score(model, x_train, y_train, cv=folds, scoring='accuracy') # roc_auc accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_func, x_train, y_train, cv_folds, k=None, results=None, v=1):\n",
    "    scores_acc = cross_val_score(model_func, x_train, y_train, cv=cv_folds, scoring='accuracy') # roc_auc accuracy\n",
    "    if results is not None:\n",
    "        results[k] = scores_acc\n",
    "    if v:\n",
    "        print('scores per fold ', scores_acc)\n",
    "        print('  mean score    ', np.mean(scores_acc))\n",
    "        print('  standard dev. ', np.std(scores_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "scores per fold  [1. 1. 1. 1. 1.]\n",
      "  mean score     1.0\n",
      "  standard dev.  0.0\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "models = {\n",
    "#           'Logit': sklearn.linear_model.LogisticRegression(solver='liblinear',\n",
    "#                                                            multi_class='ovr'),\n",
    "# #           'SGD': sklearn.linear_model.SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3),\n",
    "# #           'SVC auto': sklearn.svm.SVC(gamma='auto'), \n",
    "#           'SVC': sklearn.svm.SVC(kernel='linear'), \n",
    "# #           'SVC polynomial': sklearn.svm.SVC(kernel='poly', gamma='auto', degree=4),    \n",
    "          'Decision Tree':  sklearn.tree.DecisionTreeClassifier(),\n",
    "#           'KNN 5': sklearn.neighbors.KNeighborsClassifier(n_neighbors=5),\n",
    "# #           'KNN 10': sklearn.neighbors.KNeighborsClassifier(n_neighbors=10),\n",
    "#           'Ensemble Random Forest': sklearn.ensemble.RandomForestClassifier(n_estimators=100),\n",
    "# #           'Ensemble Bagging': sklearn.ensemble.BaggingClassifier(n_estimators=100)\n",
    "#             'GradBoost': sklearn.ensemble.GradientBoostingRegressor(loss='ls', learning_rate=0.1, \n",
    "#                             n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "#                             max_depth=3,random_state=seed, alpha=0.9, tol=0.0001)    \n",
    "# #          'AdaBoost': sklearn.ensemble.AdaBoostRegressor()\n",
    "         }\n",
    "\n",
    "results = {}\n",
    "for k,m in models.items():\n",
    "    print(k)\n",
    "    cross_validation(m, x_train, y_train, folds, k, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Mean & Std. dev. \\\\ \n",
      "\\hline\n",
      "Decision Tree & 1.0000 & 0.0000\\\\\n",
      "\n",
      "best acc: Decision Tree 1.0\n"
     ]
    }
   ],
   "source": [
    "# render latex table\n",
    "print('Model & Mean & Std. dev. \\\\\\\\ \\n\\\\hline')\n",
    "best_k = ''\n",
    "best_mean = 0\n",
    "for k, scores_acc in results.items():\n",
    "    if np.mean(scores_acc) > best_mean:\n",
    "        best_mean = np.mean(scores_acc)\n",
    "        best_k = k\n",
    "    print('%s & %0.4f & %0.4f\\\\\\\\' % (k, np.mean(scores_acc), np.std(scores_acc)))\n",
    "print('\\nbest acc:', best_k, round(best_mean,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
