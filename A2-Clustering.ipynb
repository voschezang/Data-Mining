{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "import collections\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD, NormalPredictor\n",
    "# from surprise import evaluate\n",
    "import surprise.model_selection\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn import preprocessing\n",
    "import sklearn.cluster\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import optimizers, backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.plot\n",
    "import util.data\n",
    "from util import clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(util.plot)\n",
    "reload(util.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 85)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = pd.read_csv('data/training_set_VU_DM_clean.csv', sep=';', nrows=50*1000)\n",
    "util.data.rm_na(data_all)\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_test = util.data.train_test_split(data_all)\n",
    "\n",
    "# split cross validation folds\n",
    "folds = util.data.cv_folds_for_sklearn(data, n_cv_folds=3, resampling_ratio=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "See also https://blog.keras.io/building-autoencoders-in-keras.html and https://github.com/voschezang/drum-style-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = data_all.iloc[:10] # ['cluster_id_items_KMeans']\n",
    "# # a.loc[:, 'cluster_id_items_KMeans'] = 3\n",
    "# indices = list(a.index)\n",
    "# indices += indices\n",
    "# b = data_all.loc[indices]\n",
    "# # len(list(a.index))\n",
    "# # data_all.loc[indices].shape, len(list(indices))\n",
    "# # b.loc[indices].shape, len(indices), len(list(indices))\n",
    "# a.loc[[0,1], 'prop_brand_bool'] = [4,5]\n",
    "# data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\textract_data(k: srch_id)\n",
      "\textract_data(k: prop_id)\n",
      "\tKMeans (k: `cluster_id_users_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_users_FeatureAgglomeration`)\n",
      "\tKMeans (k: `cluster_id_items_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_items_FeatureAgglomeration`)\n",
      "cluster_id_users_KMeans cluster_id_items_KMeans\n",
      "RMSE: 0.8289\n",
      "FCP:  0.4615\n",
      "RMSE: 0.8289\n",
      "\t rmse: 0.83 , fcp: 0.46\n",
      "cluster_id_users_KMeans cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.8282\n",
      "FCP:  0.4431\n",
      "RMSE: 0.8282\n",
      "\t rmse: 0.83 , fcp: 0.44\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_KMeans\n",
      "RMSE: 0.6015\n",
      "FCP:  0.9644\n",
      "RMSE: 0.6015\n",
      "\t rmse: 0.60 , fcp: 0.96\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.6021\n",
      "FCP:  0.9645\n",
      "RMSE: 0.6021\n",
      "\t rmse: 0.60 , fcp: 0.96\n",
      "\textract_data(k: srch_id)\n",
      "\textract_data(k: prop_id)\n",
      "\tKMeans (k: `cluster_id_users_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_users_FeatureAgglomeration`)\n",
      "\tKMeans (k: `cluster_id_items_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_items_FeatureAgglomeration`)\n",
      "cluster_id_users_KMeans cluster_id_items_KMeans\n",
      "RMSE: 0.8284\n",
      "FCP:  0.4946\n",
      "RMSE: 0.8284\n",
      "\t rmse: 0.83 , fcp: 0.49\n",
      "cluster_id_users_KMeans cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.8275\n",
      "FCP:  0.4566\n",
      "RMSE: 0.8275\n",
      "\t rmse: 0.83 , fcp: 0.46\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_KMeans\n",
      "RMSE: 0.6017\n",
      "FCP:  0.9646\n",
      "RMSE: 0.6017\n",
      "\t rmse: 0.60 , fcp: 0.96\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.6021\n",
      "FCP:  0.9645\n",
      "RMSE: 0.6021\n",
      "\t rmse: 0.60 , fcp: 0.96\n",
      "\textract_data(k: srch_id)\n",
      "\textract_data(k: prop_id)\n",
      "\tKMeans (k: `cluster_id_users_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_users_FeatureAgglomeration`)\n",
      "\tKMeans (k: `cluster_id_items_KMeans`)\n",
      "\tFeatureAgglomeration (k: `cluster_id_items_FeatureAgglomeration`)\n",
      "cluster_id_users_KMeans cluster_id_items_KMeans\n",
      "RMSE: 0.8280\n",
      "FCP:  0.4705\n",
      "RMSE: 0.8280\n",
      "\t rmse: 0.83 , fcp: 0.47\n",
      "cluster_id_users_KMeans cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.8280\n",
      "FCP:  0.4518\n",
      "RMSE: 0.8280\n",
      "\t rmse: 0.83 , fcp: 0.45\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_KMeans\n",
      "RMSE: 0.6028\n",
      "FCP:  0.9637\n",
      "RMSE: 0.6028\n",
      "\t rmse: 0.60 , fcp: 0.96\n",
      "cluster_id_users_FeatureAgglomeration cluster_id_items_FeatureAgglomeration\n",
      "RMSE: 0.6021\n",
      "FCP:  0.9645\n",
      "RMSE: 0.6021\n",
      "\t rmse: 0.60 , fcp: 0.96\n"
     ]
    }
   ],
   "source": [
    "# suppress warning to improve speed\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "reload(util.data)\n",
    "reload(util.clustering)\n",
    "# cluster_id_items_KMeans\n",
    "\n",
    "cv_results = collections.defaultdict(list)\n",
    "for i_train, i_test in folds:\n",
    "    # Cluser users & items\n",
    "    keys_search, keys_property, models_user, models_item = clustering.init(data_all)\n",
    "    clustering.init_df_columns(data_all, models_user, models_item)\n",
    "    xy_train = data_all.loc[i_train]\n",
    "#     clustering.train(xy_train, keys_search, keys_property, models_user, models_item)\n",
    "    clustering.fit(xy_train, models_user, keys_search,'srch_id')\n",
    "    clustering.fit(xy_train, models_item, keys_property,'prop_id')\n",
    "\n",
    "    users = clustering.predict(data_all, models_user, keys_search,\n",
    "                 'srch_id', clustering.USER_KEY_PREFIX)\n",
    "    items = clustering.predict(data_all, models_item, keys_property,\n",
    "                 'prop_id', clustering.ITEM_KEY_PREFIX)\n",
    "\n",
    "    for k in users.columns:\n",
    "        util.data.replace_most_uncommon(users, k)\n",
    "        data_all.loc[users.index, k] = users[k]\n",
    "    for k in items.columns:\n",
    "        util.data.replace_most_uncommon(items, k)\n",
    "        data_all.loc[items.index, k] = items[k]\n",
    "    \n",
    "    assert not items.isna().any().any()\n",
    "    \n",
    "    # train SVD's\n",
    "\n",
    "    # check all combinations\n",
    "    for k_user in users.columns:\n",
    "        for k_item in items.columns:\n",
    "            print(k_user, k_item)\n",
    "            assert not data_all[k_user].isna().any()\n",
    "            assert not data_all[k_item].isna().any()            \n",
    "            \n",
    "            scores = util.data.scores_df(data_all, k_user, k_item)\n",
    "            # check minimal occurence\n",
    "            value_counts_user = scores.user.value_counts(ascending=True)\n",
    "            value_counts_item = scores.item.value_counts(ascending=True)\n",
    "            assert value_counts_user.iloc[0] > 2, value_counts_user # use more data?\n",
    "            assert value_counts_item.iloc[0] > 2, value_counts_item\n",
    "            \n",
    "            reader = Reader(rating_scale=(0,6))\n",
    "            data_scores = Dataset.load_from_df(scores, reader)\n",
    "            # surprise.model_selection.cross_validate(SVD(), data_scores, cv=5)\n",
    "            # model = SVD(**gs.best_params['rmse'])\n",
    "            model = SVD()\n",
    "            trainset, testset = surprise.model_selection.train_test_split(data_scores, test_size=0.3, random_state=seed)\n",
    "            model.fit(trainset)\n",
    "            predictions = model.test(testset)\n",
    "            rmse,fcp = surprise.accuracy.rmse(predictions), surprise.accuracy.fcp(predictions)\n",
    "            cv_results[k_user + '-' + k_item].append(surprise.accuracy.rmse(predictions))\n",
    "#             cv_result[k_user + '-' + k_item]['fcp'].append(surprise.accuracy.fcp(predictions))\n",
    "            print('\\t rmse: %0.2f , fcp: %0.2f' % (rmse, fcp))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(util.data)\n",
    "# i_train, i_test = folds[0]\n",
    "# xy_train = data_all.loc[i_train]\n",
    "# if True:\n",
    "#     # check all combinations\n",
    "#     for k_user in users.columns:\n",
    "#         for k_item in items.columns:\n",
    "#             print(k_user, k_item)\n",
    "#             scores = util.data.scores_df(xy_train, k_user, k_item)\n",
    "#             # check minimal occurence\n",
    "#             value_counts_user = scores.user.value_counts(ascending=True)\n",
    "#             value_counts_item = scores.item.value_counts(ascending=True)\n",
    "#             assert value_counts_user.iloc[0] > 2 # use more data?\n",
    "#             assert value_counts_item.iloc[0] > 2\n",
    "            \n",
    "#             reader = Reader(rating_scale=(0,6))\n",
    "#             data_scores = Dataset.load_from_df(scores, reader)\n",
    "#             # surprise.model_selection.cross_validate(SVD(), data_scores, cv=5)\n",
    "#             # model = SVD(**gs.best_params['rmse'])\n",
    "#             model = SVD()\n",
    "#             trainset, testset = surprise.model_selection.train_test_split(data_scores, test_size=0.3, random_state=seed)\n",
    "#             model.fit(trainset)\n",
    "#             predictions = model.test(testset)\n",
    "#             rmse,fcp = surprise.accuracy.rmse(predictions), surprise.accuracy.fcp(predictions)\n",
    "#             cv_result[k_user + '-' + k_item].append(surprise.accuracy.rmse(predictions))\n",
    "# #             cv_result[k_user + '-' + k_item]['fcp'].append(surprise.accuracy.fcp(predictions))\n",
    "#             print('\\t rmse: %0.2f , fcp: %0.2f' % (rmse, fcp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id_users_KMeans-cluster_id_items_KMeans & 0.828 & 0.0004\n",
      "cluster_id_users_KMeans-cluster_id_items_FeatureAgglomeration & 0.828 & 0.0003\n",
      "cluster_id_users_FeatureAgglomeration-cluster_id_items_KMeans & 0.602 & 0.0006\n",
      "cluster_id_users_FeatureAgglomeration-cluster_id_items_FeatureAgglomeration & 0.602 & 0.0000\n"
     ]
    }
   ],
   "source": [
    "for k, values in cv_result.items():\n",
    "    print('%s & %0.4f & %0.4f' % (k,np.mean(values), np.std(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    results = collections.defaultdict(dict)\n",
    "    for item, user, score in testset:\n",
    "        result = model.predict(str(item), str(user), verbose=0)\n",
    "        results[user][item] = result.est\n",
    "    return results\n",
    "# surprise.SVD.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pred = predict(model, testset)\n",
    "# scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kmeans_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-3a92d7e34d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys_search\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mitem_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys_property\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kmeans_model' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO use original user/item ids and convert results to csv\n",
    "# i.e. \n",
    "result = collections.defaultdict(dict)\n",
    "for row in 'train.csv':\n",
    "    user_id = kmeans_model.predict(row[keys_search])\n",
    "    item_id = kmeans_model.predict(row[keys_property])\n",
    "    score = predict(model, (user_id, item_id))\n",
    "    result[user_id][item_id] = score\n",
    "    \n",
    "# then convert result dict to df with columns (user id, prop id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surprise.model_selection.cross_validate(SVD(), data_scores, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.toarray()\n",
    "# y_train = y_train[:,:,np.newaxis]\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.random.random(x_train.shape)\n",
    "# y_train = np.random.random(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1)\n"
     ]
    }
   ],
   "source": [
    "# n_dims = data.shape[1]\n",
    "input_shape = x_train.shape[1:]\n",
    "output_shape = x_train.shape[1:]\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=input_shape)\n",
    "h = encoder_input\n",
    "h = Flatten()(h)\n",
    "h = Dense(100, activation='relu')(h)\n",
    "# decoder\n",
    "h = Dense(np.prod(output_shape), activation='relu')(h)\n",
    "h = Reshape(output_shape)(h)\n",
    "encoder_output = Activation('sigmoid')(h)\n",
    "model = Model(encoder_input, encoder_output, name='ae-')\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy') # RMSprop Adadelta SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 1), (35, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 1s 33ms/step - loss: -35.1065 - val_loss: -38.4029\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 433us/step - loss: -35.2843 - val_loss: -38.4029\n"
     ]
    }
   ],
   "source": [
    "# validation_split does not shuffle the data\n",
    "result = model.fit(x_train, x_train, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "# history = result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [-38.40291213989258, -38.40291213989258],\n",
       " 'loss': [-35.10652494430542, -35.28428649902344]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.params\n",
    "result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae, encoder, decoder = init_vae(input_shape, output_shape, n_latent_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 35, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               3600      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 35)                3535      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 35, 1)             0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 35, 1)             0         \n",
      "=================================================================\n",
      "Total params: 7,135\n",
      "Trainable params: 7,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, to_file='model.pdf', show_shapes=True, rankdir='TB') # TB LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First design an AE as simple as possible, to avoid overfitting\n",
    "# I.e. reduce the number of nodes until the (train) performance drops\n",
    "# Then increase the complexity (i.e. number of layers) to improve the compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1) 10\n",
      "z_input\n"
     ]
    }
   ],
   "source": [
    "# see also\n",
    "# https://github.com/voschezang/drum-style-transfer/blob/master/src/models.py\n",
    "# https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
    "\n",
    "# def vae_loss(vae_input: np.ndarray,\n",
    "#              vae_output: np.ndarray,\n",
    "#              z_mean: np.ndarray,\n",
    "#              z_log_var: np.ndarray,\n",
    "#              beta=0.,\n",
    "#              extra_loss_func=keras.losses.mean_absolute_error,\n",
    "#              gamma=0.):\n",
    "#     \"\"\" Loss function for VAE\n",
    "#     beta = disentanglement amount\n",
    "#     gamma = multiplier for extra loss function\n",
    "#     \"\"\"\n",
    "# #     vae_input = K.flatten(vae_input)\n",
    "# #     vae_output = K.flatten(vae_output)\n",
    "#     n_dims = np.prod(vae_input)\n",
    "#     # bin. cross-entropy\n",
    "#     xent_loss = n_dims * keras.metrics.binary_crossentropy(vae_input, vae_output)\n",
    "#     # Kullback-Leibler divergence\n",
    "#     kl_loss = -1 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     # (optional) kl_loss = max(kl_loss, vae_input)\n",
    "#     extra_loss = n_dims * extra_loss_func(vae_input, vae_output)\n",
    "#     vae_loss = K.mean(xent_loss + beta * kl_loss + gamma * extra_loss)\n",
    "# #     vae_loss = kl_loss\n",
    "#     vae_loss = xent_loss\n",
    "# #     vae_loss = keras.losses.mean_absolute_error(vae_input, vae_output)\n",
    "#     return vae_loss\n",
    "def vae_loss(vae_input,\n",
    "             vae_output,\n",
    "             z_mean,\n",
    "             z_log_var,\n",
    "             timesteps=40,\n",
    "             notes=9,\n",
    "             beta=0.5,\n",
    "             extra_loss_f=keras.losses.mean_absolute_error,\n",
    "             gamma=0.5):\n",
    "    \"\"\"\n",
    "    vae_input, vae_output == x == y :: np.ndarray\n",
    "    z_mean, z_log_var :: np.ndarray\n",
    "    beta = disentanglement amount\n",
    "    gamma = multiplier for extra loss function\n",
    "    \"\"\"\n",
    "    vae_input_ = K.flatten(vae_input)\n",
    "    vae_output_ = K.flatten(vae_output)\n",
    "    # bin. cross-entropy\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(\n",
    "        vae_input_, vae_output_)\n",
    "    # Kullback-Leibler divergence\n",
    "    kl_loss = -1. * K.sum(\n",
    "        1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    # (optional) kl_loss = max(kl_loss, free_bits)\n",
    "    extra_loss = timesteps * notes * extra_loss_f(vae_input_, vae_output_)\n",
    "\n",
    "    # change beta\n",
    "    #     beta = ((1.0 - tf.pow(hparams.beta_rate, tf.to_float(self.global_step)))\n",
    "    #             * hparams.max_beta)\n",
    "    #     self.loss = tf.reduce_mean(r_loss) + beta * tf.reduce_mean(kl_cost)\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss + gamma * extra_loss)\n",
    "    return vae_loss\n",
    "\n",
    "\n",
    "def init_encoder(input_shape=(10,1), n_latent_dims=2):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    n_dims = np.prod(input_shape)\n",
    "    \n",
    "    # hidden layers\n",
    "    h = encoder_input\n",
    "    h = Flatten()(h)\n",
    "    for i in range(1):\n",
    "        h = Dense(100, activation='relu')(h)\n",
    "\n",
    "    # Output = Distribution of latent space z (Mean, Variance)\n",
    "    z_mean = Dense(n_latent_dims, name='z_mean')(h)  # no activation='relu'\n",
    "    z_log_var = Dense(n_latent_dims, name='z_log_var')(h)  # no activation='relu'\n",
    "\n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder = Model(encoder_input, encoder_output, name='encoder-')\n",
    "    return encoder, encoder_input, z_mean, z_log_var\n",
    "\n",
    "\n",
    "def init_decoder(output_shape=(10, 1), n_latent_dims=10):\n",
    "    \"\"\" decoder_input = z_output\n",
    "    decoder.predict :: z -> x\n",
    "    the decoder models p(x|z), with an assumption over p(z)\n",
    "    \"\"\"\n",
    "    # image_data_format = 'channels_last'\n",
    "    n_dims = output_shape\n",
    "    decoder_input = Input(shape=(n_latent_dims, ))\n",
    "    # hidden layers\n",
    "    h = decoder_input\n",
    "    for i in range(1):\n",
    "        h = Dense(100, activation='relu')(h)\n",
    "\n",
    "    h = Dense(np.prod(output_shape), activation='relu')(h)\n",
    "    h = Reshape(output_shape)(h)    \n",
    "    return Model(decoder_input, h, name='decoder-')\n",
    "\n",
    "\n",
    "def init_vae(input_shape=(10, 1), n_latent_dims=10, epsilon_std=1.):\n",
    "    output_shape = input_shape\n",
    "    # A classifier with the structure of a variational autoencoder (vae)\n",
    "    print(input_shape, output_shape, n_latent_dims)\n",
    "    encoder, encoder_input, z_mean, z_log_var = init_encoder(\n",
    "        input_shape, n_latent_dims)\n",
    "    z_input = encoder(encoder_input)\n",
    "    print('z_input')\n",
    "    \n",
    "    # this function must be defined locally in order for the model to be serializable\n",
    "    # using the following inline lambda function will result in an error when .to_json() is called\n",
    "    def sample(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(\n",
    "            shape=(K.shape(z_mean)[0], n_latent_dims),\n",
    "            mean=0.,\n",
    "            stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "    z_output = Lambda(sample)(z_input)\n",
    "    decoder = init_decoder(output_shape, n_latent_dims)\n",
    "    # VAE\n",
    "    vae_input = encoder_input\n",
    "    vae_output = decoder(z_output)\n",
    "    vae = Model(vae_input, vae_output, name='vae-')\n",
    "    loss = vae_loss(vae_input, vae_output, z_mean, z_log_var)\n",
    "#     loss = keras.losses.binary_crossentropy(vae_input, vae_output)\n",
    "    vae.add_loss(loss)\n",
    "    vae.compile(optimizer='adam') # binary_crossentropy\n",
    "#     vae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    encoder_deterministic = Model(encoder_input, z_mean)\n",
    "    return vae, encoder_deterministic, decoder\n",
    "\n",
    "vae, encoder, decoder = init_vae(input_shape, n_latent_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[[17.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[29.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[31.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       ...,\n\n       [[71.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[61.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[ 6.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 1.]]]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f5b9244cb176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# result = vae.fit(x, x, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# result = vae.fit(x, x, epochs=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     61\u001b[0m             raise ValueError('Error when checking model ' +\n\u001b[1;32m     62\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                              'expected no data, but got:', data)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[[17.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[29.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[31.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       ...,\n\n       [[71.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[61.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[ 6.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 1.]]]))"
     ]
    }
   ],
   "source": [
    "# result = vae.fit(x_train, y_train, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "x = np.random.random(x_train.shape)\n",
    "# result = vae.fit(x, x, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "# result = vae.fit(x, x, epochs=2)\n",
    "result = vae.fit(x_train, x_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3433886 ],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [ 0.62094724]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.predict(np.random.random(x_train.shape)[:2])[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.predict(x_train[:2] * 0.3)[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isinf(x_train)), np.any(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 10),\n",
       " array([-278.16318 ,  277.88397 , -123.55464 , -426.1635  ,  -53.177402],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = encoder.predict(x_train[:2])\n",
    "z.shape, z[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.        ],\n",
       "       [ -0.        ],\n",
       "       [  0.73194695],\n",
       "       [ -0.        ],\n",
       "       [127.27972   ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.predict(z)[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*BrwllztzxZO89qCB9w5d-w.png\" style=\"width:50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
