{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "import collections, copy, pickle\n",
    "from termcolor import colored\n",
    "from importlib import reload\n",
    "from dateutil.parser import parse\n",
    "import scipy.linalg, scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 14\n",
    "# rcParams['text.usetex'] = True\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, NormalPredictor\n",
    "# from surprise import evaluate\n",
    "import surprise.model_selection\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "import sklearn.linear_model\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn import preprocessing\n",
    "import sklearn.cluster\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import optimizers, backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.plot\n",
    "import util.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(util.plot)\n",
    "reload(util.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 134), 40)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/training_set_VU_DM_clean.csv', sep=';', nrows=1*1000)\n",
    "data_test = pd.read_csv('data/test_set_VU_DM.csv', sep=',', nrows=1000)\n",
    "data.shape, data.srch_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = preprocessing.KBinsDiscretizer(n_bins=3, encode='onehot', strategy='quantile')\n",
    "# x = pd.DataFrame({'A':np.arange(9), 'B':np.random.random(9)})\n",
    "# z = x.A.values.reshape(-1,1)\n",
    "# m.fit(z)\n",
    "# y = m.transform(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # type(np.random.random(2))\n",
    "# isinstance(np.random.random(2), np.ndarray)\n",
    "# pd.DataFrame(y)\n",
    "# pd.SparseArray(y)\n",
    "# y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in data.columns:\n",
    "    if data[k].isna().sum() > 0:\n",
    "        print('rm ', k)\n",
    "        data.drop(columns=[k], inplace=True)\n",
    "\n",
    "varsUsed = list(data.columns)\n",
    "# varsUsed.remove('Unnamed')\n",
    "# varsUsed.remove('position')\n",
    "# varsUsed.remove('random_bool')\n",
    "len(varsUsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>position</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>...</th>\n",
       "      <th>srch_booking_window_label0</th>\n",
       "      <th>srch_booking_window_label1</th>\n",
       "      <th>srch_booking_window_label2</th>\n",
       "      <th>srch_booking_window_label3</th>\n",
       "      <th>srch_booking_window_label4</th>\n",
       "      <th>prop_log_historical_price_bin0</th>\n",
       "      <th>prop_log_historical_price_bin1</th>\n",
       "      <th>prop_log_historical_price_bin2</th>\n",
       "      <th>prop_log_historical_price_bin3</th>\n",
       "      <th>prop_log_historical_price_bin4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>893</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.025862</td>\n",
       "      <td>-0.344778</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.047673</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.297414</td>\n",
       "      <td>-0.553668</td>\n",
       "      <td>26</td>\n",
       "      <td>0.578822</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21315</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.297414</td>\n",
       "      <td>-0.484279</td>\n",
       "      <td>21</td>\n",
       "      <td>0.664862</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.025862</td>\n",
       "      <td>-0.571016</td>\n",
       "      <td>34</td>\n",
       "      <td>4.681671</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29604</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.107759</td>\n",
       "      <td>0.235634</td>\n",
       "      <td>4</td>\n",
       "      <td>0.320893</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id  visitor_hist_starrating  visitor_hist_adr_usd  prop_id  \\\n",
       "0        1                      0.0                   0.0      893   \n",
       "1        1                      0.0                   0.0    10404   \n",
       "2        1                      0.0                   0.0    21315   \n",
       "3        1                      0.0                   0.0    27348   \n",
       "4        1                      0.0                   0.0    29604   \n",
       "\n",
       "   prop_review_score  prop_brand_bool  prop_location_score1  \\\n",
       "0               -0.5                1             -0.025862   \n",
       "1                0.0                1             -0.297414   \n",
       "2                0.5                1             -0.297414   \n",
       "3                0.0                1             -0.025862   \n",
       "4               -0.5                1             -0.107759   \n",
       "\n",
       "   prop_location_score2  position  price_usd               ...                \\\n",
       "0             -0.344778        27  -0.047673               ...                 \n",
       "1             -0.553668        26   0.578822               ...                 \n",
       "2             -0.484279        21   0.664862               ...                 \n",
       "3             -0.571016        34   4.681671               ...                 \n",
       "4              0.235634         4   0.320893               ...                 \n",
       "\n",
       "   srch_booking_window_label0  srch_booking_window_label1  \\\n",
       "0                           1                           0   \n",
       "1                           1                           0   \n",
       "2                           1                           0   \n",
       "3                           1                           0   \n",
       "4                           1                           0   \n",
       "\n",
       "   srch_booking_window_label2  srch_booking_window_label3  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   srch_booking_window_label4  prop_log_historical_price_bin0  \\\n",
       "0                           0                             0.0   \n",
       "1                           0                             0.0   \n",
       "2                           0                             0.0   \n",
       "3                           0                             1.0   \n",
       "4                           0                             0.0   \n",
       "\n",
       "   prop_log_historical_price_bin1  prop_log_historical_price_bin2  \\\n",
       "0                             0.0                             1.0   \n",
       "1                             0.0                             1.0   \n",
       "2                             0.0                             1.0   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             1.0   \n",
       "\n",
       "   prop_log_historical_price_bin3  prop_log_historical_price_bin4  \n",
       "0                             0.0                             0.0  \n",
       "1                             0.0                             0.0  \n",
       "2                             0.0                             0.0  \n",
       "3                             0.0                             0.0  \n",
       "4                             0.0                             0.0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "See also https://blog.keras.io/building-autoencoders-in-keras.html and https://github.com/voschezang/drum-style-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k in data.columns if 'comp' not in k and 'prop' not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74,\n",
       " ['srch_id',\n",
       "  'visitor_hist_starrating',\n",
       "  'visitor_hist_adr_usd',\n",
       "  'promotion_flag',\n",
       "  'srch_saturday_night_bool',\n",
       "  'srch_query_affinity_score',\n",
       "  'orig_destination_distance',\n",
       "  'srch_person_per_room_score',\n",
       "  'srch_adults_per_room_score',\n",
       "  'delta_starrating',\n",
       "  'visitor_hist_adr_usd_log',\n",
       "  'price_usd_log',\n",
       "  'has_purch_hist_bool',\n",
       "  'has_historical_price',\n",
       "  'year',\n",
       "  'month',\n",
       "  'day',\n",
       "  'hour',\n",
       "  'minute',\n",
       "  'weekday _label0',\n",
       "  'weekday _label1',\n",
       "  'weekday _label2',\n",
       "  'weekday _label3',\n",
       "  'weekday _label4',\n",
       "  'weekday _label5',\n",
       "  'weekday _label6',\n",
       "  'site_id_label0',\n",
       "  'site_id_label1',\n",
       "  'site_id_label2',\n",
       "  'site_id_label3',\n",
       "  'site_id_label4',\n",
       "  'site_id_label5',\n",
       "  'site_id_label6',\n",
       "  'site_id_label7',\n",
       "  'site_id_label8',\n",
       "  'site_id_label9',\n",
       "  'visitor_location_country_id_label0',\n",
       "  'visitor_location_country_id_label1',\n",
       "  'visitor_location_country_id_label2',\n",
       "  'visitor_location_country_id_label3',\n",
       "  'visitor_location_country_id_label4',\n",
       "  'visitor_location_country_id_label5',\n",
       "  'visitor_location_country_id_label6',\n",
       "  'visitor_location_country_id_label7',\n",
       "  'visitor_location_country_id_label8',\n",
       "  'visitor_location_country_id_label9',\n",
       "  'srch_destination_id_label0',\n",
       "  'srch_destination_id_label1',\n",
       "  'srch_destination_id_label2',\n",
       "  'srch_destination_id_label3',\n",
       "  'srch_destination_id_label4',\n",
       "  'srch_destination_id_label5',\n",
       "  'srch_destination_id_label6',\n",
       "  'srch_destination_id_label7',\n",
       "  'srch_destination_id_label8',\n",
       "  'srch_destination_id_label9',\n",
       "  'srch_adults_count_label0',\n",
       "  'srch_adults_count_label1',\n",
       "  'srch_adults_count_label2',\n",
       "  'srch_adults_count_label3',\n",
       "  'srch_adults_count_label4',\n",
       "  'srch_children_count_label0',\n",
       "  'srch_children_count_label1',\n",
       "  'srch_children_count_label2',\n",
       "  'srch_room_count_label0',\n",
       "  'srch_length_of_stay_label0',\n",
       "  'srch_length_of_stay_label1',\n",
       "  'srch_length_of_stay_label2',\n",
       "  'srch_length_of_stay_label3',\n",
       "  'srch_booking_window_label0',\n",
       "  'srch_booking_window_label1',\n",
       "  'srch_booking_window_label2',\n",
       "  'srch_booking_window_label3',\n",
       "  'srch_booking_window_label4'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rm irrelevant info\n",
    "keys_property = [k for k in data.columns if 'comp' in k or 'prop' in k]\n",
    "keys_property += ['click_bool', 'booking_bool', 'gross_bookings_usd', 'random_bool', 'score', 'price_usd', 'position', 'travel_distance']\n",
    "keys_search = [k for k in data.columns if k not in keys_property]\n",
    "len(keys_search), keys_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prop_id',\n",
       " 'prop_review_score',\n",
       " 'prop_brand_bool',\n",
       " 'prop_location_score1',\n",
       " 'prop_location_score2',\n",
       " 'unavailable_comp',\n",
       " 'available_comp',\n",
       " 'avg_price_comp',\n",
       " 'prop_country_id_label0',\n",
       " 'prop_country_id_label1',\n",
       " 'prop_country_id_label2',\n",
       " 'prop_country_id_label3',\n",
       " 'prop_country_id_label4',\n",
       " 'prop_country_id_label5',\n",
       " 'prop_country_id_label6',\n",
       " 'prop_country_id_label7',\n",
       " 'prop_country_id_label8',\n",
       " 'prop_country_id_label9',\n",
       " 'prop_starrating_label0',\n",
       " 'prop_starrating_label1',\n",
       " 'prop_starrating_label2',\n",
       " 'prop_starrating_label3',\n",
       " 'prop_starrating_label4',\n",
       " 'prop_starrating_label5',\n",
       " 'prop_log_historical_price_bin0',\n",
       " 'prop_log_historical_price_bin1',\n",
       " 'prop_log_historical_price_bin2',\n",
       " 'prop_log_historical_price_bin3',\n",
       " 'prop_log_historical_price_bin4',\n",
       " 'click_bool',\n",
       " 'booking_bool',\n",
       " 'gross_bookings_usd',\n",
       " 'random_bool',\n",
       " 'score',\n",
       " 'price_usd',\n",
       " 'position',\n",
       " 'travel_distance']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change order\n",
    "keys_property[0], keys_property[1] = keys_property[1], keys_property[0]\n",
    "assert keys_property[0] == 'prop_id', 'incorrect order'\n",
    "keys_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data, keys, k='srch_id'):\n",
    "    data = data[keys]\n",
    "#     data[k].unique()\n",
    "    data_unique_rows = data.drop_duplicates(subset=k)\n",
    "    assert data_unique_rows.shape[0] == data[k].unique().size, \\\n",
    "        'if key (srch_id) is equal, all non-property attributes should be equal as well'\n",
    "\n",
    "    assert data_unique_rows[k].min() > 0, 'search id must be positive and nonzero'\n",
    "    assert ~data_unique_rows[k].isna().any()\n",
    "    assert ~data_unique_rows.isna().any().any()\n",
    "    assert ~data_unique_rows.isin([np.nan, np.inf, -np.inf]).any().any()\n",
    "    return data_unique_rows\n",
    "\n",
    "def sample(data, keys, k='srch_id', rm_first_column=True):\n",
    "    data_unique_rows = extract_data(data, keys, k)\n",
    "    sample = data_unique_rows.sample(frac=1, random_state=seed)\n",
    "    x_train = sample.values\n",
    "    if rm_first_column:\n",
    "        x_train = x_train[:,1:]\n",
    "#         x_train = sample.values[:,1:,np.newaxis]    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 73)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to np array & reshape\n",
    "# data_unique_srch_id = extract_data(data, keys_search, 'srch_id')\n",
    "# sample = data_unique_srch_id.sample(frac=1, random_state=seed)\n",
    "\n",
    "# x_train = sample.values[:,1:,np.newaxis]\n",
    "x_train = sample(data, keys_search, k='srch_id')\n",
    "\n",
    "# x_train = data_unique_srch_id.values\n",
    "# x_train = x_train.reshape(x_train.shape + (1,))\n",
    "# y_labels = x_train[:,0,:]\n",
    "# discretizer = preprocessing.KBinsDiscretizer(n_bins=y_labels.shape[0],\n",
    "#          encode='onehot', strategy='uniform')\n",
    "# y_train = discretizer.fit_transform(y_labels)\n",
    "x_train.shape # , y_labels.shape, y_train.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ~np.any(np.isinf(x_train))\n",
    "assert ~np.any(np.isinf(-x_train))\n",
    "assert ~np.any(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_users = x_train\n",
    "keys_property = [k for k in keys_property if k != 'travel_distance']\n",
    "x_train_items = sample(data, keys_property, k='prop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 73), (996, 35))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_users.shape, x_train_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "models_user = {'KMeans': sklearn.cluster.KMeans(n_clusters, n_jobs=2, random_state=seed)\n",
    "#                , 'Agglomerative': sklearn.cluster.AgglomerativeClustering(n_clusters)\n",
    "             }\n",
    "n_clusters = 10\n",
    "models_item = {'KMeans': sklearn.cluster.KMeans(n_clusters, n_jobs=2, random_state=seed)\n",
    "#                , 'Agglomerative': sklearn.cluster.AgglomerativeClustering(n_clusters)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,model in models_user.items():\n",
    "    model.fit(x_train_users)\n",
    "for k,model in models_item.items():\n",
    "    model.fit(x_train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = models_user['KMeans'].predict(x_train_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(util.data)\n",
    "def user_func(row, srch_id):\n",
    "    row_without_id = row[keys_search[1:]]\n",
    "    return models_user['KMeans'].predict([row_without_id])[0]\n",
    "\n",
    "def item_func(row, prop_id):\n",
    "    row_without_id = row[keys_property[1:]]\n",
    "    return models_item['KMeans'].predict([row_without_id])[0]\n",
    "\n",
    "scores = util.data.scores_df(data, user_func, item_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check minimal occurence\n",
    "value_counts_user = scores.user.value_counts(ascending=True)\n",
    "value_counts_item = scores.item.value_counts(ascending=True)\n",
    "assert value_counts_user.iloc[0] > 2\n",
    "assert value_counts_item.iloc[0] > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0,6))\n",
    "data_scores = Dataset.load_from_df(scores, reader)\n",
    "# surprise.model_selection.cross_validate(SVD(), data_scores, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966769517834428\n",
      "{'n_epochs': 5, 'lr_all': 0.005, 'reg_all': 0.6}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = surprise.model_selection.GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "gs.fit(data_scores)\n",
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0264\n",
      "FCP:  0.4645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0263579533216072, 0.46449136276391556)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = SVD(**gs.best_params['rmse'])\n",
    "model = SVD()\n",
    "\n",
    "trainset, testset = surprise.model_selection.train_test_split(data_scores, test_size=0.3, random_state=seed)\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "surprise.accuracy.rmse(predictions), surprise.accuracy.fcp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    results = collections.defaultdict(dict)\n",
    "    for item, user, score in testset:\n",
    "        result = model.predict(str(item), str(user), verbose=0)\n",
    "        results[user][item] = result.est\n",
    "    return results\n",
    "# surprise.SVD.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pred = predict(model, testset)\n",
    "# scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kmeans_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-3a92d7e34d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys_search\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mitem_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys_property\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kmeans_model' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO use original user/item ids and convert results to csv\n",
    "# i.e. \n",
    "result = collections.defaultdict(dict)\n",
    "for row in 'train.csv':\n",
    "    user_id = kmeans_model.predict(row[keys_search])\n",
    "    item_id = kmeans_model.predict(row[keys_property])\n",
    "    score = predict(model, (user_id, item_id))\n",
    "    result[user_id][item_id] = score\n",
    "    \n",
    "# then convert result dict to df with columns (user id, prop id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surprise.model_selection.cross_validate(SVD(), data_scores, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.toarray()\n",
    "# y_train = y_train[:,:,np.newaxis]\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.random.random(x_train.shape)\n",
    "# y_train = np.random.random(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1)\n"
     ]
    }
   ],
   "source": [
    "# n_dims = data.shape[1]\n",
    "input_shape = x_train.shape[1:]\n",
    "output_shape = x_train.shape[1:]\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=input_shape)\n",
    "h = encoder_input\n",
    "h = Flatten()(h)\n",
    "h = Dense(100, activation='relu')(h)\n",
    "# decoder\n",
    "h = Dense(np.prod(output_shape), activation='relu')(h)\n",
    "h = Reshape(output_shape)(h)\n",
    "encoder_output = Activation('sigmoid')(h)\n",
    "model = Model(encoder_input, encoder_output, name='ae-')\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy') # RMSprop Adadelta SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 1), (35, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 1s 33ms/step - loss: -35.1065 - val_loss: -38.4029\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 433us/step - loss: -35.2843 - val_loss: -38.4029\n"
     ]
    }
   ],
   "source": [
    "# validation_split does not shuffle the data\n",
    "result = model.fit(x_train, x_train, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "# history = result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [-38.40291213989258, -38.40291213989258],\n",
       " 'loss': [-35.10652494430542, -35.28428649902344]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.params\n",
    "result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae, encoder, decoder = init_vae(input_shape, output_shape, n_latent_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 35, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               3600      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 35)                3535      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 35, 1)             0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 35, 1)             0         \n",
      "=================================================================\n",
      "Total params: 7,135\n",
      "Trainable params: 7,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, to_file='model.pdf', show_shapes=True, rankdir='TB') # TB LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First design an AE as simple as possible, to avoid overfitting\n",
    "# I.e. reduce the number of nodes until the (train) performance drops\n",
    "# Then increase the complexity (i.e. number of layers) to improve the compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1) (35, 1) 10\n",
      "z_input\n"
     ]
    }
   ],
   "source": [
    "# see also\n",
    "# https://github.com/voschezang/drum-style-transfer/blob/master/src/models.py\n",
    "# https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
    "\n",
    "# def vae_loss(vae_input: np.ndarray,\n",
    "#              vae_output: np.ndarray,\n",
    "#              z_mean: np.ndarray,\n",
    "#              z_log_var: np.ndarray,\n",
    "#              beta=0.,\n",
    "#              extra_loss_func=keras.losses.mean_absolute_error,\n",
    "#              gamma=0.):\n",
    "#     \"\"\" Loss function for VAE\n",
    "#     beta = disentanglement amount\n",
    "#     gamma = multiplier for extra loss function\n",
    "#     \"\"\"\n",
    "# #     vae_input = K.flatten(vae_input)\n",
    "# #     vae_output = K.flatten(vae_output)\n",
    "#     n_dims = np.prod(vae_input)\n",
    "#     # bin. cross-entropy\n",
    "#     xent_loss = n_dims * keras.metrics.binary_crossentropy(vae_input, vae_output)\n",
    "#     # Kullback-Leibler divergence\n",
    "#     kl_loss = -1 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     # (optional) kl_loss = max(kl_loss, vae_input)\n",
    "#     extra_loss = n_dims * extra_loss_func(vae_input, vae_output)\n",
    "#     vae_loss = K.mean(xent_loss + beta * kl_loss + gamma * extra_loss)\n",
    "# #     vae_loss = kl_loss\n",
    "#     vae_loss = xent_loss\n",
    "# #     vae_loss = keras.losses.mean_absolute_error(vae_input, vae_output)\n",
    "#     return vae_loss\n",
    "def vae_loss(vae_input,\n",
    "             vae_output,\n",
    "             z_mean,\n",
    "             z_log_var,\n",
    "             timesteps=40,\n",
    "             notes=9,\n",
    "             beta=0.5,\n",
    "             extra_loss_f=keras.losses.mean_absolute_error,\n",
    "             gamma=0.5):\n",
    "    \"\"\"\n",
    "    vae_input, vae_output == x == y :: np.ndarray\n",
    "    z_mean, z_log_var :: np.ndarray\n",
    "    beta = disentanglement amount\n",
    "    gamma = multiplier for extra loss function\n",
    "    \"\"\"\n",
    "    vae_input_ = K.flatten(vae_input)\n",
    "    vae_output_ = K.flatten(vae_output)\n",
    "    # bin. cross-entropy\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(\n",
    "        vae_input_, vae_output_)\n",
    "    # Kullback-Leibler divergence\n",
    "    kl_loss = -1. * K.sum(\n",
    "        1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    # (optional) kl_loss = max(kl_loss, free_bits)\n",
    "    extra_loss = timesteps * notes * extra_loss_f(vae_input_, vae_output_)\n",
    "\n",
    "    # change beta\n",
    "    #     beta = ((1.0 - tf.pow(hparams.beta_rate, tf.to_float(self.global_step)))\n",
    "    #             * hparams.max_beta)\n",
    "    #     self.loss = tf.reduce_mean(r_loss) + beta * tf.reduce_mean(kl_cost)\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss + gamma * extra_loss)\n",
    "    return vae_loss\n",
    "\n",
    "\n",
    "def init_encoder(input_shape=(10,1), n_latent_dims=2):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    n_dims = np.prod(input_shape)\n",
    "    \n",
    "    # hidden layers\n",
    "    h = encoder_input\n",
    "    h = Flatten()(h)\n",
    "    for i in range(1):\n",
    "        h = Dense(100, activation='relu')(h)\n",
    "\n",
    "    # Output = Distribution of latent space z (Mean, Variance)\n",
    "    z_mean = Dense(n_latent_dims, name='z_mean')(h)  # no activation='relu'\n",
    "    z_log_var = Dense(n_latent_dims, name='z_log_var')(h)  # no activation='relu'\n",
    "\n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder = Model(encoder_input, encoder_output, name='encoder-')\n",
    "    return encoder, encoder_input, z_mean, z_log_var\n",
    "\n",
    "\n",
    "def init_decoder(output_shape=(10, 1), n_latent_dims=10):\n",
    "    \"\"\" decoder_input = z_output\n",
    "    decoder.predict :: z -> x\n",
    "    the decoder models p(x|z), with an assumption over p(z)\n",
    "    \"\"\"\n",
    "    # image_data_format = 'channels_last'\n",
    "    n_dims = output_shape\n",
    "    decoder_input = Input(shape=(n_latent_dims, ))\n",
    "    # hidden layers\n",
    "    h = decoder_input\n",
    "    for i in range(1):\n",
    "        h = Dense(100, activation='relu')(h)\n",
    "\n",
    "    h = Dense(np.prod(output_shape), activation='relu')(h)\n",
    "    h = Reshape(output_shape)(h)    \n",
    "    return Model(decoder_input, h, name='decoder-')\n",
    "\n",
    "\n",
    "def init_vae(input_shape=(10, 1), n_latent_dims=10, epsilon_std=1.):\n",
    "    output_shape = input_shape\n",
    "    # A classifier with the structure of a variational autoencoder (vae)\n",
    "    print(input_shape, output_shape, n_latent_dims)\n",
    "    encoder, encoder_input, z_mean, z_log_var = init_encoder(\n",
    "        input_shape, n_latent_dims)\n",
    "    z_input = encoder(encoder_input)\n",
    "    print('z_input')\n",
    "    \n",
    "    # this function must be defined locally in order for the model to be serializable\n",
    "    # using the following inline lambda function will result in an error when .to_json() is called\n",
    "    def sample(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(\n",
    "            shape=(K.shape(z_mean)[0], n_latent_dims),\n",
    "            mean=0.,\n",
    "            stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "    z_output = Lambda(sample)(z_input)\n",
    "    decoder = init_decoder(output_shape, n_latent_dims)\n",
    "    # VAE\n",
    "    vae_input = encoder_input\n",
    "    vae_output = decoder(z_output)\n",
    "    vae = Model(vae_input, vae_output, name='vae-')\n",
    "    loss = vae_loss(vae_input, vae_output, z_mean, z_log_var)\n",
    "#     loss = keras.losses.binary_crossentropy(vae_input, vae_output)\n",
    "    vae.add_loss(loss)\n",
    "    vae.compile(optimizer='adam') # binary_crossentropy\n",
    "#     vae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    encoder_deterministic = Model(encoder_input, z_mean)\n",
    "    return vae, encoder_deterministic, decoder\n",
    "\n",
    "vae, encoder, decoder = init_vae(input_shape, n_latent_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[[17.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[29.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[31.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       ...,\n\n       [[71.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[61.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[ 6.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 1.]]]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f5b9244cb176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# result = vae.fit(x, x, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# result = vae.fit(x, x, epochs=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     61\u001b[0m             raise ValueError('Error when checking model ' +\n\u001b[1;32m     62\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                              'expected no data, but got:', data)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[[17.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[29.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[31.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       ...,\n\n       [[71.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 1.],\n        [ 0.]],\n\n       [[61.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 0.]],\n\n       [[ 6.],\n        [ 0.],\n        [ 0.],\n        ...,\n        [ 0.],\n        [ 0.],\n        [ 1.]]]))"
     ]
    }
   ],
   "source": [
    "# result = vae.fit(x_train, y_train, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "x = np.random.random(x_train.shape)\n",
    "# result = vae.fit(x, x, epochs=2, batch_size=int(x_train.shape[0]/2), validation_split=0.2)\n",
    "# result = vae.fit(x, x, epochs=2)\n",
    "result = vae.fit(x_train, x_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3433886 ],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [ 0.62094724]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.predict(np.random.random(x_train.shape)[:2])[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.predict(x_train[:2] * 0.3)[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isinf(x_train)), np.any(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 10),\n",
       " array([-278.16318 ,  277.88397 , -123.55464 , -426.1635  ,  -53.177402],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = encoder.predict(x_train[:2])\n",
    "z.shape, z[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.        ],\n",
       "       [ -0.        ],\n",
       "       [  0.73194695],\n",
       "       [ -0.        ],\n",
       "       [127.27972   ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.predict(z)[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*BrwllztzxZO89qCB9w5d-w.png\" style=\"width:50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
